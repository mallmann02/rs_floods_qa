{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"key\": os.getenv(\"GSE_API_KEY\"),\n",
    "    \"cx\": os.getenv(\"GSE_CX\"),\n",
    "    \"gl\": \"br\",\n",
    "    \"lr\": \"lang_pt\",\n",
    "    \"q\": \"google news enchentes rio grande do sul clicrbs\",\n",
    "    \"num\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap the Google News RSS feed for the latest news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://news.google.com/rss/search?q=enchentes%25rio%25grande%25do%25sul&ceid=BR:pt-419&hl=pt-BR&gl=BR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the response, we can parse the XML and get the items\n",
    "from xml.etree import ElementTree\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_date(date:str) -> str:\n",
    "    my_date = re.findall(r\"[a-zA-Z]{3},\\s\\d\\d\\s[a-zA-Z]{3}\\s\\d{4}\", date)[0]\n",
    "    my_date = datetime.strptime(my_date, \"%a, %d %b %Y\")\n",
    "    my_date = my_date.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "def scrap_rss_news_feed(rss_feed:requests.Response) -> list[dict]:\n",
    "    parsed = rss_feed.content.decode(\"utf-8\").replace(\"\\n\", \"\")\n",
    "    root = ElementTree.fromstring(parsed)\n",
    "    result_items = root.findall(\"./channel/item\")\n",
    "    scrapped_news:list[dict] = []\n",
    "    for item in result_items:\n",
    "        scrapped_news.append({\n",
    "            \"title\": item.find(\"title\").text,\n",
    "            \"link\": item.find(\"link\").text,\n",
    "            \"description\": item.find(\"description\").text,\n",
    "            \"pubDate\": parse_date(item.find(\"pubDate\").text)\n",
    "        })\n",
    "    return scrapped_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read each news article and extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_isolated_special_chars(text):\n",
    "    # Regex pattern to match isolated special characters\n",
    "    pattern = r'(?<![a-zA-Z0-9])[^\\w\\s]|(?<=[^\\w\\s])[^\\w\\s](?![a-zA-Z0-9])'\n",
    "    # Replace isolated special characters with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_content(scrapped_news_sources:list[dict]) -> list[str]:\n",
    "    paragraphs_extracted = []\n",
    "    for news_source in scrapped_news_sources:\n",
    "        r = requests.get(news_source[\"link\"])\n",
    "        hmtl = r.content.decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(hmtl, \"html.parser\")\n",
    "        # filter to only collect paragraphs\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        for p in paragraphs:\n",
    "            p_text = remove_isolated_special_chars(p.text.strip())\n",
    "            p_text_words = p_text.split(\" \")\n",
    "            if p_text and len(p_text_words) > 10:\n",
    "                paragraphs_extracted.append(p_text)\n",
    "    return paragraphs_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading with HTML2Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "h.ignore_images = True\n",
    "h.ignore_emphasis = True\n",
    "h.ignore_tables = True\n",
    "h.ignore_anchors = True\n",
    "h.ignore_backrefs = True\n",
    "\n",
    "print(h.handle(hmtl.decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try filtering the data by the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "\n",
    "llm = HuggingFaceHub(huggingfacehub_api_token ='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',\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.0001,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.invoke(\"Can you tell me a joke?\", temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "context_definition = \"You are a news reviewer. You have been tasked to filter news about floods in Rio Grande do Sul and its associated impacts. Your task is filter the context for the user.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "The context below is a news article about floods in Rio Grande do Sul coming from a scraped news website. Please filter the context to only include the paragraphs related to the floods in Rio Grande do Sul and its impacts.\n",
    "\n",
    "<<CONTEXT>>\n",
    "The floods in Rio Grande do Sul have caused significant damage to the region. The floods have affected many people and have caused a lot of destruction. The floods have also caused many people to lose their homes and have caused a lot of damage to the infrastructure of the region.  \n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", context_definition),\n",
    "            (\"human\", user_prompt),\n",
    "        ])\n",
    "\n",
    "messages = template.format_messages()\n",
    "\n",
    "response = chat_model.invoke(messages, temperature=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content.split(\"</s>\")[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ChoromaDB to store the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"multi-qa-mpnet-base-cos-v1\")\n",
    "\n",
    "qa_collection = chroma_client.create_collection(\n",
    "    name=\"rs_floods_qa\",\n",
    "    embedding_function=sentence_transformer_ef,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id2', 'id0']],\n",
       " 'distances': [[0.35033607482910156, 0.355965793132782]],\n",
       " 'metadatas': [[None, None]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Ao todo, 478 municípios foram atingidos e a população afetada chega a 2.398.255 pessoas. Os municípios com o maior número de mortes até agora são Canoas 31 óbitos) Roca Sales 13) e Cruzeiro do Sul 12)',\n",
       "   'Aumentou para 176 o número de mortes confirmadas no Rio Grande do Sul em decorrência das enchentes que atingiram o estado. Um corpo não identificado foi encontrado em Venâncio Aires, na região do Vale do Rio Pardo, no centro do estado.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_collection.query(query_texts=\"Qual foi o número de mortos pelas enchentes no mês de julho?\", n_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('em Maio', datetime.datetime(2024, 5, 27, 0, 0)),\n",
       " ('ano passado', datetime.datetime(2023, 5, 27, 0, 0)),\n",
       " ('em outubro', datetime.datetime(2024, 10, 27, 0, 0))]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dateparser\n",
    "from dateparser.search import search_dates\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "dateparser.parse(\"July of 2021\")\n",
    "search_dates(\"Eu me mudei em Maio do ano passado. Ela se mudou em outubro.\", languages=[\"pt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
