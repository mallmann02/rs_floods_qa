{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import uuid\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"key\": os.getenv(\"GSE_API_KEY\"),\n",
    "    \"cx\": os.getenv(\"GSE_CX\"),\n",
    "    \"gl\": \"br\",\n",
    "    \"lr\": \"lang_pt\",\n",
    "    \"q\": \"google news enchentes rio grande do sul clicrbs\",\n",
    "    \"num\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap the Google News RSS feed for the latest news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = requests.get(\"https://news.google.com/rss/search?q=enchentes%25rio%25grande%25do%25sul&ceid=BR:pt-419&hl=pt-BR&gl=BR\")\n",
    "r2 = requests.get(\"https://news.google.com/rss/search?q=cidade%25atingidas%25pelas%25enchentes%25no%25RS&ceid=BR:pt-419&hl=pt-BR&gl=BR\")\n",
    "r3 = requests.get(\"https://news.google.com/rss/search?q=chuvas%25no%25rio%25grande%25do%25sul&ceid=BR:pt-419&hl=pt-BR&gl=BR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the response, we can parse the XML and get the items\n",
    "from xml.etree import ElementTree\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_date(date:str) -> str:\n",
    "    my_date = re.findall(r\"[a-zA-Z]{3},\\s\\d\\d\\s[a-zA-Z]{3}\\s\\d{4}\", date)[0]\n",
    "    my_date = datetime.strptime(my_date, \"%a, %d %b %Y\")\n",
    "    my_date = my_date.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "def scrap_rss_news_feed(rss_feeds:list[requests.Response]) -> list[dict]:\n",
    "    scrapped_news:list[dict] = []\n",
    "    for rss_feed in rss_feeds:\n",
    "        parsed = rss_feed.content.decode(\"utf-8\").replace(\"\\n\", \"\")\n",
    "        root = ElementTree.fromstring(parsed)\n",
    "        result_items = root.findall(\"./channel/item\")\n",
    "        for item in result_items[:50]:\n",
    "            scrapped_news.append({\n",
    "                \"title\": item.find(\"title\").text,\n",
    "                \"link\": item.find(\"link\").text,\n",
    "                \"description\": item.find(\"description\").text,\n",
    "                \"pubDate\": parse_date(item.find(\"pubDate\").text)\n",
    "            })\n",
    "    return scrapped_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read each news article and extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    # Regex pattern to match isolated special characters\n",
    "    isolatedSpecialCharacters = r'(?<![a-zA-Z0-9])[^\\w\\s]|(?<=[^\\w\\s])[^\\w\\s](?![a-zA-Z0-9])'\n",
    "    \n",
    "    \n",
    "    cleaned_text = re.sub(isolatedSpecialCharacters, '', text)\n",
    "    cleaned_text = re.sub(r'\\s\\s+', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Publicação: 24/06/2024 às 18h53min'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanText('Publicação: \\n\\r\\n            24/06/2024 às 18h53min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_content(scrapped_news_sources:list[dict]) -> list[str]:\n",
    "    paragraphs_extracted = []\n",
    "    for index, news_source in enumerate(scrapped_news_sources):\n",
    "        print(f\"Scraping {index+1}/{len(scrapped_news_sources)}\")\n",
    "        try:\n",
    "            r = requests.get(news_source[\"link\"], timeout=5)\n",
    "            hmtl = r.content.decode(\"utf-8\")\n",
    "            soup = BeautifulSoup(hmtl, \"html.parser\")\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout error on {news_source['link']}\")\n",
    "            continue\n",
    "        except ConnectionError:\n",
    "            print(f\"Connection error on {news_source['link']}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            continue\n",
    "        # filter to only collect paragraphs\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        for p in paragraphs:\n",
    "            p_text = cleanText(p.text)\n",
    "            p_text_words = p_text.split(\" \")\n",
    "            if p_text and len(p_text_words) > 10:\n",
    "                paragraphs_extracted.append(p_text)\n",
    "    return paragraphs_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading with HTML2Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "h.ignore_images = True\n",
    "h.ignore_emphasis = True\n",
    "h.ignore_tables = True\n",
    "h.ignore_anchors = True\n",
    "h.ignore_backrefs = True\n",
    "\n",
    "print(h.handle(hmtl.decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try filtering the data by the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "\n",
    "llm = HuggingFaceHub(huggingfacehub_api_token ='',\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.0001,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.invoke(\"Can you tell me a joke?\", temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "context_definition = \"You are a news reviewer. You have been tasked to filter news about floods in Rio Grande do Sul and its associated impacts. Your task is filter the context for the user.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "The context below is a news article about floods in Rio Grande do Sul coming from a scraped news website. Please filter the context to only include the paragraphs related to the floods in Rio Grande do Sul and its impacts.\n",
    "\n",
    "<<CONTEXT>>\n",
    "The floods in Rio Grande do Sul have caused significant damage to the region. The floods have affected many people and have caused a lot of destruction. The floods have also caused many people to lose their homes and have caused a lot of damage to the infrastructure of the region.  \n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", context_definition),\n",
    "            (\"human\", user_prompt),\n",
    "        ])\n",
    "\n",
    "messages = template.format_messages()\n",
    "\n",
    "response = chat_model.invoke(messages, temperature=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content.split(\"</s>\")[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ChoromaDB to store the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"multi-qa-mpnet-base-cos-v1\")\n",
    "\n",
    "qa_collection = chroma_client.create_collection(\n",
    "    name=\"rs_floods_qa\",\n",
    "    embedding_function=sentence_transformer_ef,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1/150\n",
      "Scraping 2/150\n",
      "Scraping 3/150\n",
      "Scraping 4/150\n",
      "Scraping 5/150\n",
      "Scraping 6/150\n",
      "Scraping 7/150\n",
      "Scraping 8/150\n",
      "Scraping 9/150\n",
      "Scraping 10/150\n",
      "Scraping 11/150\n",
      "Scraping 12/150\n",
      "Scraping 13/150\n",
      "Scraping 14/150\n",
      "Scraping 15/150\n",
      "Scraping 16/150\n",
      "Scraping 17/150\n",
      "Scraping 18/150\n",
      "Scraping 19/150\n",
      "Scraping 20/150\n",
      "Scraping 21/150\n",
      "Scraping 22/150\n",
      "Scraping 23/150\n",
      "Scraping 24/150\n",
      "Scraping 25/150\n",
      "Scraping 26/150\n",
      "Scraping 27/150\n",
      "Scraping 28/150\n",
      "Scraping 29/150\n",
      "Scraping 30/150\n",
      "Scraping 31/150\n",
      "Scraping 32/150\n",
      "Scraping 33/150\n",
      "Scraping 34/150\n",
      "Scraping 35/150\n",
      "Scraping 36/150\n",
      "Scraping 37/150\n",
      "Scraping 38/150\n",
      "Scraping 39/150\n",
      "Scraping 40/150\n",
      "Scraping 41/150\n",
      "Scraping 42/150\n",
      "Scraping 43/150\n",
      "Scraping 44/150\n",
      "Scraping 45/150\n",
      "Scraping 46/150\n",
      "Scraping 47/150\n",
      "Scraping 48/150\n",
      "Scraping 49/150\n",
      "Scraping 50/150\n",
      "Scraping 51/150\n",
      "Scraping 52/150\n",
      "Scraping 53/150\n",
      "Scraping 54/150\n",
      "Scraping 55/150\n",
      "Scraping 56/150\n",
      "Scraping 57/150\n",
      "Scraping 58/150\n",
      "Scraping 59/150\n",
      "Scraping 60/150\n",
      "Scraping 61/150\n",
      "Scraping 62/150\n",
      "Scraping 63/150\n",
      "Scraping 64/150\n",
      "Scraping 65/150\n",
      "Scraping 66/150\n",
      "Scraping 67/150\n",
      "Scraping 68/150\n",
      "Scraping 69/150\n",
      "Scraping 70/150\n",
      "Scraping 71/150\n",
      "Scraping 72/150\n",
      "Scraping 73/150\n",
      "Scraping 74/150\n",
      "Scraping 75/150\n",
      "An error occurred: HTTPSConnectionPool(host='www.defesacivil.rs.gov.br', port=443): Max retries exceeded with url: /primeiro-lote-de-beneficio-do-volta-por-cima-para-atingidos-pelas-enchentes-e-pago-nesta-sexta-feira-17 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\n",
      "Scraping 76/150\n",
      "Scraping 77/150\n",
      "Scraping 78/150\n",
      "Scraping 79/150\n",
      "Scraping 80/150\n",
      "Scraping 81/150\n",
      "Scraping 82/150\n",
      "Scraping 83/150\n",
      "Scraping 84/150\n",
      "Scraping 85/150\n",
      "Scraping 86/150\n",
      "Scraping 87/150\n",
      "Scraping 88/150\n",
      "Scraping 89/150\n",
      "Scraping 90/150\n",
      "Scraping 91/150\n",
      "Scraping 92/150\n",
      "Scraping 93/150\n",
      "Scraping 94/150\n",
      "Scraping 95/150\n",
      "Scraping 96/150\n",
      "Scraping 97/150\n",
      "Scraping 98/150\n",
      "Scraping 99/150\n",
      "Scraping 100/150\n",
      "Scraping 101/150\n",
      "Scraping 102/150\n",
      "Scraping 103/150\n",
      "Scraping 104/150\n",
      "Scraping 105/150\n",
      "Scraping 106/150\n",
      "Scraping 107/150\n",
      "Scraping 108/150\n",
      "Scraping 109/150\n",
      "Scraping 110/150\n",
      "Scraping 111/150\n",
      "Scraping 112/150\n",
      "Scraping 113/150\n",
      "Scraping 114/150\n",
      "Scraping 115/150\n",
      "Scraping 116/150\n",
      "Scraping 117/150\n",
      "Scraping 118/150\n",
      "Scraping 119/150\n",
      "Scraping 120/150\n",
      "Scraping 121/150\n",
      "Scraping 122/150\n",
      "Scraping 123/150\n",
      "Scraping 124/150\n",
      "Scraping 125/150\n",
      "Scraping 126/150\n",
      "Scraping 127/150\n",
      "Scraping 128/150\n",
      "Scraping 129/150\n",
      "Scraping 130/150\n",
      "Scraping 131/150\n",
      "Scraping 132/150\n",
      "Scraping 133/150\n",
      "Scraping 134/150\n",
      "Scraping 135/150\n",
      "Scraping 136/150\n",
      "Scraping 137/150\n",
      "Scraping 138/150\n",
      "Scraping 139/150\n",
      "Scraping 140/150\n",
      "Scraping 141/150\n",
      "Scraping 142/150\n",
      "Scraping 143/150\n",
      "Scraping 144/150\n",
      "Scraping 145/150\n",
      "Scraping 146/150\n",
      "Scraping 147/150\n",
      "Scraping 148/150\n",
      "Scraping 149/150\n",
      "Scraping 150/150\n"
     ]
    }
   ],
   "source": [
    "news_sources = scrap_rss_news_feed([r1, r2, r3])\n",
    "documents = get_news_content(news_sources)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_collection.add(\n",
    "    id=\"id1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id2', 'id0']],\n",
       " 'distances': [[0.35033607482910156, 0.355965793132782]],\n",
       " 'metadatas': [[None, None]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Ao todo, 478 municípios foram atingidos e a população afetada chega a 2.398.255 pessoas. Os municípios com o maior número de mortes até agora são Canoas 31 óbitos) Roca Sales 13) e Cruzeiro do Sul 12)',\n",
       "   'Aumentou para 176 o número de mortes confirmadas no Rio Grande do Sul em decorrência das enchentes que atingiram o estado. Um corpo não identificado foi encontrado em Venâncio Aires, na região do Vale do Rio Pardo, no centro do estado.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_collection.query(query_texts=\"Qual foi o número de mortos pelas enchentes no mês de julho?\", n_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('em Maio', datetime.datetime(2024, 5, 27, 0, 0)),\n",
       " ('ano passado', datetime.datetime(2023, 5, 27, 0, 0)),\n",
       " ('em outubro', datetime.datetime(2024, 10, 27, 0, 0))]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dateparser\n",
    "from dateparser.search import search_dates\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "dateparser.parse(\"July of 2021\")\n",
    "search_dates(\"Eu me mudei em Maio do ano passado. Ela se mudou em outubro.\", languages=[\"pt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
