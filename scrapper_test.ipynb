{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import uuid\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"key\": os.getenv(\"GSE_API_KEY\"),\n",
    "    \"cx\": os.getenv(\"GSE_CX\"),\n",
    "    \"gl\": \"br\",\n",
    "    \"lr\": \"lang_pt\",\n",
    "    \"q\": \"google news enchentes rio grande do sul clicrbs\",\n",
    "    \"num\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap the Google News RSS feed for the latest news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = requests.get(\"https://news.google.com/rss/search?q=enchentes%25rio%25grande%25do%25sul&ceid=BR:pt-419&hl=pt-BR&gl=BR\")\n",
    "r2 = requests.get(\"https://news.google.com/rss/search?q=cidade%25atingidas%25pelas%25enchentes%25no%25RS&ceid=BR:pt-419&hl=pt-BR&gl=BR\")\n",
    "r3 = requests.get(\"https://news.google.com/rss/search?q=chuvas%25no%25rio%25grande%25do%25sul&ceid=BR:pt-419&hl=pt-BR&gl=BR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the response, we can parse the XML and get the items\n",
    "from xml.etree import ElementTree\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_date(date:str) -> str:\n",
    "    my_date = re.findall(r\"[a-zA-Z]{3},\\s\\d\\d\\s[a-zA-Z]{3}\\s\\d{4}\", date)[0]\n",
    "    my_date = datetime.strptime(my_date, \"%a, %d %b %Y\")\n",
    "    my_date = my_date.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "def scrap_rss_news_feed(rss_feeds:list[requests.Response]) -> list[dict]:\n",
    "    scrapped_news:list[dict] = []\n",
    "    for rss_feed in rss_feeds:\n",
    "        parsed = rss_feed.content.decode(\"utf-8\").replace(\"\\n\", \"\")\n",
    "        root = ElementTree.fromstring(parsed)\n",
    "        result_items = root.findall(\"./channel/item\")\n",
    "        for item in result_items[:50]:\n",
    "            scrapped_news.append({\n",
    "                \"title\": item.find(\"title\").text,\n",
    "                \"link\": item.find(\"link\").text,\n",
    "                \"description\": item.find(\"description\").text,\n",
    "                \"pubDate\": parse_date(item.find(\"pubDate\").text)\n",
    "            })\n",
    "    return scrapped_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read each news article and extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    # Regex pattern to match isolated special characters\n",
    "    isolatedSpecialCharacters = r'(?<![a-zA-Z0-9])[^\\w\\s]|(?<=[^\\w\\s])[^\\w\\s](?![a-zA-Z0-9])'\n",
    "    \n",
    "    \n",
    "    cleaned_text = re.sub(isolatedSpecialCharacters, '', text)\n",
    "    cleaned_text = re.sub(r'\\s\\s+', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText('Publicação: \\n\\r\\n            24/06/2024 às 18h53min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_content(scrapped_news_sources:list[dict]) -> list[str]:\n",
    "    paragraphs_extracted = []\n",
    "    for index, news_source in enumerate(scrapped_news_sources):\n",
    "        print(f\"Scraping {index+1}/{len(scrapped_news_sources)}\")\n",
    "        try:\n",
    "            r = requests.get(news_source[\"link\"], timeout=5)\n",
    "            hmtl = r.content.decode(\"utf-8\")\n",
    "            soup = BeautifulSoup(hmtl, \"html.parser\")\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout error on {news_source['link']}\")\n",
    "            continue\n",
    "        except ConnectionError:\n",
    "            print(f\"Connection error on {news_source['link']}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            continue\n",
    "        # filter to only collect paragraphs\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        for p in paragraphs:\n",
    "            p_text = cleanText(p.text)\n",
    "            p_text_words = p_text.split(\" \")\n",
    "            if p_text and len(p_text_words) > 10:\n",
    "                paragraphs_extracted.append(p_text)\n",
    "    return paragraphs_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading with HTML2Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "h.ignore_images = True\n",
    "h.ignore_emphasis = True\n",
    "h.ignore_tables = True\n",
    "h.ignore_anchors = True\n",
    "h.ignore_backrefs = True\n",
    "\n",
    "print(h.handle(hmtl.decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try filtering the data by the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "\n",
    "llm = HuggingFaceHub(huggingfacehub_api_token ='',\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.0001,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.invoke(\"Can you tell me a joke?\", temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "context_definition = \"You are a news reviewer. You have been tasked to filter news about floods in Rio Grande do Sul and its associated impacts. Your task is filter the context for the user.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "The context below is a news article about floods in Rio Grande do Sul coming from a scraped news website. Please filter the context to only include the paragraphs related to the floods in Rio Grande do Sul and its impacts.\n",
    "\n",
    "<<CONTEXT>>\n",
    "The floods in Rio Grande do Sul have caused significant damage to the region. The floods have affected many people and have caused a lot of destruction. The floods have also caused many people to lose their homes and have caused a lot of damage to the infrastructure of the region.  \n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", context_definition),\n",
    "            (\"human\", user_prompt),\n",
    "        ])\n",
    "\n",
    "messages = template.format_messages()\n",
    "\n",
    "response = chat_model.invoke(messages, temperature=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content.split(\"</s>\")[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ChoromaDB to store the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"multi-qa-mpnet-base-cos-v1\")\n",
    "\n",
    "qa_collection = chroma_client.create_collection(\n",
    "    name=\"rs_floods_qa\",\n",
    "    embedding_function=sentence_transformer_ef,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sources = scrap_rss_news_feed([r1, r2, r3])\n",
    "documents = get_news_content(news_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [str(uuid.uuid4())[-8:] for _ in range(len(documents))]\n",
    "\n",
    "qa_collection.add(\n",
    "    ids=ids,\n",
    "    documents=documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_collection.query(query_texts=\"Qual foi o número de pessoas mortas pelas enchentes?\", n_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateparser\n",
    "from dateparser.search import search_dates\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "dateparser.parse(\"July of 2021\")\n",
    "search_dates(\"Eu me mudei em Maio do ano passado. Ela se mudou em outubro.\", languages=[\"pt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(\"Abalada por três vezes pela enchente do Rio Taquari desde setembro do ano passado, a cidade de Roca Sales, no Vale do Taquari, prioriza a reconstrução a cidade por duas frentes: habitação e infraestrutura. De acordo com o prefeito de Roca Sales, Amilton Fontana, por conta das inundações há a necessidade de 400 residências e a desobstrução de estradas e vias públicas.\")\n",
    "\n",
    "# search for consecutive PROPN entities\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"JONES\", \"pattern\": [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "# tem que substituir o NER oficial do pipe\n",
    "\n",
    "doc = nlp(\"Abalada por três vezes pela enchente do Rio Taquari desde setembro do ano passado, a cidade de Roca Sales, no Vale do Taquari, prioriza a reconstrução a cidade por duas frentes: habitação e infraestrutura. De acordo com o prefeito de Roca Sales, Amilton Fontana, por conta das inundações há a necessidade de 400 residências e a desobstrução de estradas e vias públicas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc:\n",
    "    print(ent.text, ent.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "text = \"João mora na Bahia, 22/11/1985, seu cpf é 111.222.333-11\"\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [\n",
    "    {\"label\": \"JONES\", \"pattern\": [\n",
    "            {\"SHAPE\": \"ddd.ddd.\"},\n",
    "            {\"SHAPE\": \"ddd-dd\"},\n",
    "    ]},\n",
    "    {\"label\": \"NOME\", \"pattern\": [\n",
    "            {\"POS\": \"PROPN\"},\n",
    "    ]},\n",
    "    {\"label\": \"DATA\", \"pattern\": [\n",
    "            {\"SHAPE\": \"dd/dd/dddd\"},\n",
    "            {\"SHAPE\": \"dd/dd/dddd\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
